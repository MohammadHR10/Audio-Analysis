{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91380b84-e9ea-4fde-acaf-23552c042792",
   "metadata": {},
   "source": [
    "# 🧠 Cycle 2 Week 6 – Part 2  \n",
    "## Assignment: “What Are We Really Hearing?” – Voice, Sound, and Cultural Signal Analysis in Short-Form Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9dcd4-bf87-457b-936f-03846be83412",
   "metadata": {},
   "source": [
    "## 📘 Assignment Narrative\n",
    "\n",
    "Short-form videos aren’t just visual. They’re compressed cultural objects, encoding emotion, performance, and group belonging in milliseconds.\n",
    "\n",
    "Where Part 1 explored what we’re really seeing, this notebook focuses on the audio layer — what we hear, and what that sound implies. Because often, what something sounds like carries just as much weight as what’s actually said.\n",
    "\n",
    "This week, our exploratory notebook builds toward a broader goal: to interpret videos in ways humans do, using not just audio content but tone, cadence, slang, musical choice, and delivery style to surface signals of belonging, trend alignment, and identity performance.\n",
    "\n",
    "Content is also interpreted differently by different audiences — a phenomenon sometimes used intentionally to share in-group jokes, evade moderation, or pass cultural signals under the radar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be8476-dac2-4384-adad-3ae6126b59fb",
   "metadata": {},
   "source": [
    "## 🔎 Why This Matters\n",
    "\n",
    "Modern tools for content recommendation, indexing, and moderation can’t rely on explicit tags anymore.\n",
    "\n",
    "Instead, they need to understand:\n",
    "\n",
    "- How slang travels from fringe groups to the middle  \n",
    "- How cadence and tone give away social or emotional intent  \n",
    "- How music or speaking rhythm might index a subculture  \n",
    "- How communication fingerprinting (like yours!) works — not just in *what’s said*, but *how*\n",
    "\n",
    "If this sounds like we're building a classifier, we are — but also, we’re building a **listener**:\n",
    "tuned to the invisible layer of cultural and emotional structure embedded in milliseconds of sound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461de968-5f18-4664-9052-bd2bce80b797",
   "metadata": {},
   "source": [
    "## 🔧 What We Are Doing\n",
    "\n",
    "In this assignment, we’re building a pipeline that:\n",
    "- Extracts the audio track from a short-form video  \n",
    "- Transcribes speech using an ASR transformer (e.g., Whisper)  \n",
    "- Analyzes musical structure (tempo, beat, tone)  \n",
    "- Detects emerging slang or trending phrases  \n",
    "- Measures emotional energy in delivery  \n",
    "- Builds a communication fingerprint (pacing, rhythm, lexical density)  \n",
    "- Interprets voice as a cultural and emotional signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b14b9-4e97-4074-a82d-379737a1ad4d",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "\n",
    "✅ Learn how to extract and process audio for machine interpretation  \n",
    "✅ Understand the acoustic correlates of emotion and rhythm  \n",
    "✅ Practice transcription and keyword extraction from short, noisy clips  \n",
    "✅ Explore how identity and group signals are encoded in voice cadence  \n",
    "✅ Build a pipeline that begins to interpret sound as **social signal**, not just information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a8336-ac0b-434d-9118-9d7e55a99408",
   "metadata": {},
   "source": [
    "## 🟦 1. Setup & Dependencies\n",
    "\n",
    "Environment config, packages: `whisper`, `librosa`, `torch`, `nltk`, etc.\n",
    "\n",
    "We begin by configuring our environment and installing the necessary libraries:\n",
    "\n",
    "- `whisper`: OpenAI's transformer-based ASR model, pretrained on multilingual audio-text pairs\n",
    "- `librosa`: Used for audio feature extraction (MFCCs, tempo, pitch)\n",
    "- `moviepy`: For slicing video files and extracting audio tracks\n",
    "- `nltk`: To tokenize and analyze text content from transcripts\n",
    "- `torch`: Required as the backend for running Whisper and tensor computations\n",
    "\n",
    "This setup prepares the computational tools we'll use to simulate human-like interpretation of sound, rhythm, and cultural signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440eeeda-105e-4ed1-a2d7-7f577f376780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/mistune-0.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/mistune-0.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/mistune-0.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/mistune-0.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/mistune-0.8.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires jax>=0.3.15, which is not installed.\n",
      "tensorflow 2.12.0 requires libclang>=13.0.0, which is not installed.\n",
      "d2l 1.0.3 requires matplotlib==3.7.2, but you have matplotlib 3.8.4 which is incompatible.\n",
      "d2l 1.0.3 requires numpy==1.23.5, but you have numpy 2.0.2 which is incompatible.\n",
      "d2l 1.0.3 requires pandas==2.0.3, but you have pandas 2.2.3 which is incompatible.\n",
      "d2l 1.0.3 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "d2l 1.0.3 requires scipy==1.10.1, but you have scipy 1.13.1 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.0.2 which is incompatible.\n",
      "torchtext 0.17.2 requires torch==2.2.2, but you have torch 2.1.0 which is incompatible.\n",
      "streamlit 1.32.0 requires numpy<2,>=1.19.3, but you have numpy 2.0.2 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/bq/_yk5zsn90t11g7lt1yn4v9w40000gn/T/ipykernel_13554/2635618208.py\", line 3, in <module>\n",
      "    import whisper\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/whisper/__init__.py\", line 8, in <module>\n",
      "    import torch\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/torch/__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/torch/functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/bq/_yk5zsn90t11g7lt1yn4v9w40000gn/T/ipykernel_13554/2635618208.py\", line 3, in <module>\n",
      "    import whisper\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/whisper/__init__.py\", line 13, in <module>\n",
      "    from .model import ModelDimensions, Whisper\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/whisper/model.py\", line 14, in <module>\n",
      "    from .transcribe import transcribe as transcribe_function\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/whisper/transcribe.py\", line 21, in <module>\n",
      "    from .timing import add_word_timestamps\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/whisper/timing.py\", line 7, in <module>\n",
      "    import numba\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/numba/__init__.py\", line 92, in <module>\n",
      "    from numba.core.decorators import (cfunc, jit, njit, stencil,\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/numba/core/decorators.py\", line 12, in <module>\n",
      "    from numba.stencils.stencil import stencil\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/numba/stencils/stencil.py\", line 11, in <module>\n",
      "    from numba.core import types, typing, utils, ir, config, ir_utils, registry\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/numba/core/ir_utils.py\", line 14, in <module>\n",
      "    from numba.core.extending import _Intrinsic\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/numba/core/extending.py\", line 19, in <module>\n",
      "    from numba.core.pythonapi import box, unbox, reflect, NativeValue  # noqa: F401\n",
      "  File \"/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/numba/core/pythonapi.py\", line 11, in <module>\n",
      "    from numba import _helperlib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install -q openai-whisper librosa moviepy nltk torch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meditor\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/whisper/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_audio, log_mel_spectrogram, pad_or_trim\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecodingOptions, DecodingResult, decode, detect_language\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelDimensions, Whisper\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranscribe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transcribe\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/whisper/model.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decode \u001b[38;5;28;01mas\u001b[39;00m decode_function\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect_language \u001b[38;5;28;01mas\u001b[39;00m detect_language_function\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranscribe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transcribe \u001b[38;5;28;01mas\u001b[39;00m transcribe_function\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scaled_dot_product_attention\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/whisper/transcribe.py:21\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     FRAMES_PER_SECOND,\n\u001b[1;32m     13\u001b[0m     HOP_LENGTH,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     pad_or_trim,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecodingOptions, DecodingResult\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtiming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_word_timestamps\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LANGUAGES, TO_LANGUAGE_CODE, get_tokenizer\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     exact_div,\n\u001b[1;32m     25\u001b[0m     format_timestamp,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     str2bool,\n\u001b[1;32m     32\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/whisper/timing.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, List\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numba/__init__.py:92\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Re-export decorators\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (cfunc, jit, njit, stencil,\n\u001b[1;32m     93\u001b[0m                                    jit_module)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Re-export vectorize decorators and the thread layer querying function\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mufunc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (vectorize, guvectorize, threading_layer,\n\u001b[1;32m     97\u001b[0m                             get_num_threads, set_num_threads,\n\u001b[1;32m     98\u001b[0m                             set_parallel_chunksize, get_parallel_chunksize,\n\u001b[1;32m     99\u001b[0m                             get_thread_id)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numba/core/decorators.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeprecationError, NumbaDeprecationWarning\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstencils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstencil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stencil\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, extending, sigutils, registry\n\u001b[1;32m     15\u001b[0m _logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numba/stencils/stencil.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllvmlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ir \u001b[38;5;28;01mas\u001b[39;00m lir\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types, typing, utils, ir, config, ir_utils, registry\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemplates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (CallableTemplate, signature,\n\u001b[1;32m     13\u001b[0m                                          infer_global, AbstractTemplate)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lower_builtin\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numba/core/ir_utils.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextending\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _Intrinsic\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types, typing, ir, analysis, postproc, rewrites, config\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemplates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signature\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numba/core/extending.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models   \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_default \u001b[38;5;28;01mas\u001b[39;00m register_model  \u001b[38;5;66;03m# noqa: F401, E501\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpythonapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m box, unbox, reflect, NativeValue  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_helperlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _import_cython_function  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceMixin\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numba/core/pythonapi.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllvmlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mir\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Constant\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _helperlib\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     types, utils, config, lowering, cgutils, imputils, serialize,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PYVERSION\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "!pip install -q openai-whisper librosa moviepy nltk torch\n",
    "\n",
    "import whisper\n",
    "import librosa\n",
    "import moviepy.editor as mp\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "# Confirm GPU availability\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d5019-84a7-49ef-ab80-4f741b9e763f",
   "metadata": {},
   "source": [
    "## 🟦 2. Audio Track Extraction\n",
    "\n",
    "Before we can analyze rhythm, emotion, or lexical content, we must isolate the audio signal from the video file.\n",
    "\n",
    "We use `moviepy` to extract the audio stream and save it as a `.wav` file — an uncompressed format suitable for downstream processing:\n",
    "\n",
    "- `.wav` files preserve **raw signal fidelity**, making them ideal for MFCC and tempo analysis  \n",
    "- `pcm_s16le` is a standard **16-bit linear PCM** codec, widely compatible and non-lossy  \n",
    "- This step simulates the first action of a human listener: tuning in to the sound, separate from the visuals\n",
    "\n",
    "Once saved, this audio can be passed to transcription, rhythm analysis, and acoustic modeling pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d987e1c2-fe56-4e34-8343-f00b16c46cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def extract_audio(video_path, output_path='audio.wav'):\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(output_path, codec='pcm_s16le')\n",
    "    return output_path\n",
    "\n",
    "# Example usage\n",
    "audio_path = extract_audio(\"your_video.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefce2a-2f4b-4135-b581-b044870451ca",
   "metadata": {},
   "source": [
    "### 🧮 Signal Preprocessing\n",
    "\n",
    "Why we convert to mono, fixed sample rate, and normalize for MFCCs.\n",
    "\n",
    "> To ensure consistent feature extraction, audio signals must be standardized. Mono conversion removes stereo imbalances; a fixed sample rate ensures consistent frequency resolution; normalization reduces loudness variance which otherwise biases spectral measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd5bbf-ceee-4420-ab70-859ca2a01a8e",
   "metadata": {},
   "source": [
    "## 🟦 3. Transcription with Whisper\n",
    "\n",
    "Once the audio has been extracted, the next step is to understand *what is being said* — not just how it sounds. For this, we use OpenAI’s `whisper`, a state-of-the-art automatic speech recognition (ASR) model.\n",
    "\n",
    "Whisper is built using a Transformer encoder-decoder architecture:\n",
    "\n",
    "- The audio signal is first converted into a **mel-spectrogram**, a frequency-based visual representation\n",
    "- An encoder processes this spectrogram to create contextual embeddings\n",
    "- A decoder then generates text tokens autoregressively, attending to both audio context and previously decoded tokens\n",
    "\n",
    "This model is robust to accents, background noise, and informal or fast-paced speech — making it ideal for analyzing short-form, often messy social video content.\n",
    "\n",
    "Transcription provides both the **verbal content** and (optionally) the **timestamps**, which can later help align tone, tempo, and emotion with specific phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe6061-21ad-4cec-a56b-e66f5701c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# Load the Whisper model (you can choose 'tiny', 'base', 'small', etc.)\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Transcribe the audio file\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "# Print the raw transcription\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d01d4-3ec6-4a80-a26e-6f8ed50140a0",
   "metadata": {},
   "source": [
    "### 🧮 Transformer-Based ASR\n",
    "\n",
    "Explain how Whisper tokenizes audio into text, decoder logic, model structure.\n",
    "\n",
    "> Whisper uses a multi-lingual transformer architecture. It converts raw audio into mel-spectrograms, which are then interpreted via attention blocks and decoded autoregressively to generate text, optionally with timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc2b16-2e6b-4e7f-a1ec-2facfc329f52",
   "metadata": {},
   "source": [
    "## 🟦 4. Acoustic Feature Analysis\n",
    "\n",
    "To move beyond words and into *tone*, we extract acoustic features that shape our emotional perception of speech.\n",
    "\n",
    "Using `librosa`, we calculate:\n",
    "\n",
    "- **MFCCs** (Mel-Frequency Cepstral Coefficients): Represent voice timbre, correlating with emotional texture (e.g., warmth, tension)  \n",
    "- **Tempo**: Overall pacing or beat frequency, often aligned with energy or urgency  \n",
    "- **Pitch Contour**: Variation in fundamental frequency over time, shaping expressiveness and intonation\n",
    "\n",
    "These low-level features mirror how humans infer mood or intent from sound — whether someone is rushed, relaxed, angry, or cheerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833e339-bc08-4d69-9644-9b1f292b882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# Load the audio\n",
    "y, sr = librosa.load(audio_path)\n",
    "\n",
    "# Extract MFCCs\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "\n",
    "# Estimate tempo (beats per minute)\n",
    "tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "# Estimate pitch using the YIN algorithm\n",
    "pitch = librosa.yin(y, fmin=50, fmax=300)\n",
    "\n",
    "# Output summaries\n",
    "print(\"MFCC shape:\", mfccs.shape)\n",
    "print(\"Estimated Tempo (BPM):\", tempo)\n",
    "print(\"Pitch Contour (sample):\", pitch[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9394f-e66e-4731-a6f6-912b3cd36f13",
   "metadata": {},
   "source": [
    "### 🧮 Spectral Features as Mood Markers\n",
    "\n",
    "Grounded in **speech prosody theory**, these features have real psychological correlates:\n",
    "\n",
    "- **MFCCs** capture fine-grained variations in resonance and vocal tract shape — markers of emotional coloring  \n",
    "- **Tempo** often reflects arousal (fast = excited or anxious, slow = calm or sad)  \n",
    "- **Pitch movement** signals affective variation (monotone vs melodic speech)\n",
    "\n",
    "Together, these help machines listen not just to *what* is said, but *how* it’s said."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c00f0d-3f1a-42af-8807-23f1416e31b8",
   "metadata": {},
   "source": [
    "## 🟦 5. Emotion Detection from Voice\n",
    "\n",
    "Human listeners often infer emotion from how something is said, not just the words themselves. Machines can mimic this using acoustic proxies for emotional modulation.\n",
    "\n",
    "Here, we use three primary features:\n",
    "\n",
    "- **Pitch** (frequency contour): Variation in fundamental frequency correlates with emotional expressiveness (e.g., rising pitch = excitement or surprise)  \n",
    "- **Tempo** (speech rate): Faster tempo typically indicates urgency, agitation, or enthusiasm; slower pace often signals calmness or sadness  \n",
    "- **Volume** (amplitude envelope): Loudness relates to arousal or emphasis  \n",
    "\n",
    "We use these to classify an approximate emotional state on the Russell Circumplex Model — a 2D affective space where:\n",
    "\n",
    "- The **x-axis** reflects **valence** (pleasant → unpleasant)\n",
    "- The **y-axis** reflects **energy level** (calm → excited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8eefff-fea7-4055-959d-caa500e699f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a first approximation — \n",
    "# more sophisticated emotion models would use classifiers \n",
    "# trained on labeled corpora (e.g., RAVDESS or CREMA-D).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Compute basic stats\n",
    "pitch_mean = np.mean(pitch)\n",
    "pitch_std = np.std(pitch)\n",
    "volume = np.sqrt(np.mean(y**2))  # RMS energy\n",
    "\n",
    "# Basic heuristics for emotion classification\n",
    "if pitch_std > 30 and tempo > 100 and volume > 0.02:\n",
    "    emotion = \"Excited / Happy\"\n",
    "elif pitch_std < 10 and tempo < 80:\n",
    "    emotion = \"Calm / Sad\"\n",
    "elif pitch_std > 20 and tempo > 90:\n",
    "    emotion = \"Energetic / Angry\"\n",
    "else:\n",
    "    emotion = \"Neutral or Uncertain\"\n",
    "\n",
    "print(\"Estimated Emotion Tone:\", emotion)\n",
    "print(f\"Pitch Std Dev: {pitch_std:.2f} | Tempo: {tempo:.2f} | Volume (RMS): {volume:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597569e7-793a-40f3-80e4-74468c3455ae",
   "metadata": {},
   "source": [
    "### 🧮 Affect from Acoustic Modulation\n",
    "\n",
    "Let:\n",
    "- $ f_0(t) $: fundamental frequency (pitch) over time  \n",
    "- $ E(t) $: energy or volume (e.g., RMS amplitude)  \n",
    "- $ B(t) $: beat rate or tempo  \n",
    "\n",
    "Then:\n",
    "- High $ \\sigma(f_0(t)) $ → expressive / dynamic speech (emotionally intense)\n",
    "- High $ B(t) $ → elevated arousal (e.g., anger, joy)\n",
    "- Low $ B(t) $, low $ f_0(t) $ variation → subdued, flat affect (e.g., boredom, sadness)\n",
    "\n",
    "> Emotional expression in speech can be modeled via pitch contour (excitement), tempo (urgency), volume (arousal), and dynamic range (engagement).  \n",
    "This offers a simple yet powerful lens for decoding tone and social signal from voice alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551ea03-6922-43a6-af13-2ec95ff1de09",
   "metadata": {},
   "source": [
    "## 🟦 6. Slang and Keyword Drift\n",
    "\n",
    "Extract n-grams / trends from Whisper transcript.\n",
    "\n",
    "Slang, memes, and emergent phrases move from fringe groups to the mainstream through repeated exposure, imitation, and algorithmic amplification. These changes are often reflected first in the *sound* of language, before appearing in formal lexicons.\n",
    "\n",
    "By analyzing short word sequences (n-grams) in our transcripts, we can start to detect:\n",
    "\n",
    "- Repetitive or trending phrases\n",
    "- Semantic clusters that reflect cultural reference points\n",
    "- Phrase structures that map to genre, subculture, or moment\n",
    "\n",
    "This process lays the groundwork for identifying *what kind of thing this video is trying to be* — whether that’s a skit, a joke, an aesthetic statement, or a participation in a meme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a039e70-28fb-471a-b701-3acaf41bf053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Download tokenizer resources if not already present\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize and lowercase the transcript\n",
    "tokens = nltk.word_tokenize(result[\"text\"].lower())\n",
    "\n",
    "# Extract bigrams (or try trigrams for more structure)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "# Count and display top patterns\n",
    "trending = Counter(bigrams).most_common(10)\n",
    "print(\"Top Trending Bigrams:\")\n",
    "for phrase, count in trending:\n",
    "    print(f\"{' '.join(phrase)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c5bc1-12c3-481d-ae04-624b17156055",
   "metadata": {},
   "source": [
    "### 🧮 Social Lexicon Drift\n",
    "\n",
    "Let $ T = \\{w_1, w_2, \\dots, w_n\\} $ be the tokenized transcript.\n",
    "\n",
    "We compute $ n $-grams:\n",
    "\n",
    "$$\n",
    "\\text{NGram}(T, n) = \\{(w_i, w_{i+1}, \\dots, w_{i+n-1}) \\mid i = 1, \\dots, n - k + 1\\}\n",
    "$$\n",
    "\n",
    "and evaluate their **burstiness**, **frequency**, and **collocation strength**:\n",
    "\n",
    "- **Burstiness**: How sharply a phrase rises in popularity  \n",
    "- **PMI (Pointwise Mutual Information)**: Measures strength of association between terms  \n",
    "- **TF-IDF / trend curves**: For longer horizon drift tracking\n",
    "\n",
    "This gives us a signal of *lexical diffusion* — how novel expressions bubble up in different social strata and transition from noise to signal.\n",
    "\n",
    "> “What are people saying?” becomes: “What *kind* of saying is this becoming?”\n",
    "\n",
    "Analyzing these structures helps index emergent genres, humor, identity, and performance — all core to decoding modern media."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3584fb-7c9d-44fc-8b90-06e5a1ef5e93",
   "metadata": {},
   "source": [
    "## 🟦 7. Cadence + Delivery Pattern Analysis\n",
    "\n",
    "Word per second, pause frequency, filler usage, stutter, “Millennial pause.”\n",
    "\n",
    "Cadence — the rhythm of delivery — plays a major role in how identity and emotion are expressed through voice. It acts as a kind of \"social accent\" that reveals:\n",
    "\n",
    "- **Pacing** (e.g., rushed, calm, deliberate)\n",
    "- **Pause patterns** (e.g., hesitation, rhetorical pauses)\n",
    "- **Filler words** (e.g., “like,” “uh,” “you know”)\n",
    "- **Delivery style** (e.g., the \"Millennial pause\" — a purposeful breath before speaking)\n",
    "\n",
    "These are not just quirks — they are part of what sociolinguists call **paralinguistic signaling**, contributing to perceived authenticity, genre alignment, and group belonging.\n",
    "\n",
    "Cadence helps answer:  \n",
    "> “Does this person sound like a YouTuber? A streamer? A parody? A real person?”  \n",
    "\n",
    "It also relates to how short-form platforms reward or suppress certain styles (e.g., speed-talking for algorithm favor, deadpan for humor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a0154-e59b-4ef3-8da6-2febaa8c6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Estimate total audio duration in seconds\n",
    "duration_sec = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "# Count total words\n",
    "words = len(tokens)\n",
    "\n",
    "# Compute words per second (speech rate)\n",
    "wps = words / duration_sec\n",
    "print(f\"Words per second (WPS): {wps:.2f}\")\n",
    "\n",
    "# Optional: Detect filler words (basic example set)\n",
    "filler_words = {'um', 'uh', 'like', 'you know', 'so', 'actually'}\n",
    "filler_count = sum(token in filler_words for token in tokens)\n",
    "filler_ratio = filler_count / words if words > 0 else 0\n",
    "\n",
    "print(f\"Filler word ratio: {filler_ratio:.3f} ({filler_count} fillers in {words} words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcea23c-9d8a-444f-9216-94fbeb62ef80",
   "metadata": {},
   "source": [
    "### 🧮 Speech Rhythm as Identity Marker\n",
    "\n",
    "Sociolinguistics of pacing and paralinguistic cues.\n",
    "\n",
    "> Timing patterns (e.g., delayed starts, filler interjections) reveal speaker identity and group alignment. Pacing correlates with personality traits and cultural affinity.\n",
    "\n",
    "Let:\n",
    "- $ N $: total number of transcribed words  \n",
    "- $ D $: total duration of the audio in seconds  \n",
    "- $ F $: number of known filler words (e.g., from a curated set)\n",
    "\n",
    "Then:\n",
    "- Words per second: $ WPS = \\frac{N}{D} $\n",
    "- Filler ratio: $ \\frac{F}{N} $\n",
    "\n",
    "We also track pause duration between words, and staccato vs flowing delivery. These rhythm signatures can become **latent features** in downstream classifiers (e.g., genre, sentiment, persona).\n",
    "\n",
    "> In essence, we are fingerprinting the *musicality of speech*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d378f64-b112-4ad3-9ef5-adfa5ed907aa",
   "metadata": {},
   "source": [
    "## 🟦 8. Cultural Fingerprinting Summary\n",
    "\n",
    "After analyzing voice tone, rhythm, lexical choices, and acoustic mood, we arrive at a key question:\n",
    "\n",
    "> What kind of person — or performance — is this?\n",
    "\n",
    "This final section synthesizes:\n",
    "\n",
    "- 🗣️ **Emotion from voice** (pitch, tempo, volume)\n",
    "- 📈 **Delivery style** (cadence, filler use, “Millennial pause”)\n",
    "- 💬 **Lexical drift** (slang, memes, genre phrases)\n",
    "- 🎵 **Acoustic aesthetic** (energy, warmth, musicality)\n",
    "\n",
    "These features combine into a **soft fingerprint** — a flexible, human-like impression of *how someone talks* and *who they sound like they are*. This may hint at:\n",
    "\n",
    "- Subculture (e.g., gaming, K-pop, alt-literature)\n",
    "- Role or intent (e.g., educational, ironic, sincere)\n",
    "- Identity signals (e.g., age group, in-group, parody)\n",
    "\n",
    "In short: We’re modeling **performance style** — a key component of how social video is interpreted by both people and algorithms.\n",
    "\n",
    "> The same phrase can mean very different things depending on *how* it's said. And now, our model can begin to sense that difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
